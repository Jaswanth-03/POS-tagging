{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBa417SvSfzq3lg6Yf7IFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaswanth-03/POS-tagging/blob/main/POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, GRU, Bidirectional, Dropout, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import treebank, indian\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "\n",
        "# Download the required NLTK datasets\n",
        "nltk.download('treebank')\n",
        "nltk.download('indian')\n",
        "\n",
        "# Data Preparation for English\n",
        "english_sentences = treebank.tagged_sents()\n",
        "english_pos_tags = [[tag for _, tag in sent] for sent in english_sentences]\n",
        "english_words = [[word for word, _ in sent] for sent in english_sentences]\n",
        "\n",
        "# Tokenize words and POS tags for English\n",
        "english_words_flat = [word for sublist in english_words for word in sublist]\n",
        "english_pos_tags_flat = [tag for sublist in english_pos_tags for tag in sublist]\n",
        "\n",
        "english_word_vocab = set(english_words_flat)\n",
        "english_pos_vocab = set(english_pos_tags_flat)\n",
        "\n",
        "english_word_index = {word: idx + 1 for idx, word in enumerate(english_word_vocab)}\n",
        "english_pos_index = {pos: idx + 1 for idx, pos in enumerate(english_pos_vocab)}\n",
        "\n",
        "# Convert words and POS tags to sequences\n",
        "english_train_seq = [[english_word_index[word] for word in sent] for sent in english_words]\n",
        "english_train_pos_seq = [[english_pos_index[tag] for tag in sent] for sent in english_pos_tags]\n",
        "\n",
        "# Pad sequences for English\n",
        "english_max_len = max(len(seq) for seq in english_train_seq)\n",
        "english_train_seq = pad_sequences(english_train_seq, maxlen=english_max_len, padding='post')\n",
        "english_train_pos_seq = pad_sequences(english_train_pos_seq, maxlen=english_max_len, padding='post')\n",
        "\n",
        "# Calculate vocabulary size for English\n",
        "vocab_size_eng = len(english_word_vocab) + 1\n",
        "pos_size_eng = len(english_pos_vocab) + 1\n",
        "\n",
        "# Data Preparation for Hindi\n",
        "hindi_sentences = indian.tagged_sents('hindi.pos')\n",
        "hindi_pos_tags = [[tag for _, tag in sent] for sent in hindi_sentences]\n",
        "hindi_words = [[word for word, _ in sent] for sent in hindi_sentences]\n",
        "\n",
        "# Tokenize words and POS tags for Hindi\n",
        "hindi_words_flat = [word for sublist in hindi_words for word in sublist]\n",
        "hindi_pos_tags_flat = [tag for sublist in hindi_pos_tags for tag in sublist]\n",
        "\n",
        "hindi_word_vocab = set(hindi_words_flat)\n",
        "hindi_pos_vocab = set(hindi_pos_tags_flat)\n",
        "\n",
        "hindi_word_index = {word: idx + 1 for idx, word in enumerate(hindi_word_vocab)}\n",
        "hindi_pos_index = {pos: idx + 1 for idx, pos in enumerate(hindi_pos_vocab)}\n",
        "\n",
        "# Convert words and POS tags to sequences\n",
        "hindi_train_seq = [[hindi_word_index[word] for word in sent] for sent in hindi_words]\n",
        "hindi_train_pos_seq = [[hindi_pos_index[tag] for tag in sent] for sent in hindi_pos_tags]\n",
        "\n",
        "# Pad sequences for Hindi\n",
        "hindi_max_len = max(len(seq) for seq in hindi_train_seq)\n",
        "hindi_train_seq = pad_sequences(hindi_train_seq, maxlen=hindi_max_len, padding='post')\n",
        "hindi_train_pos_seq = pad_sequences(hindi_train_pos_seq, maxlen=hindi_max_len, padding='post')\n",
        "\n",
        "# Calculate vocabulary size for Hindi\n",
        "vocab_size_hindi = len(hindi_word_vocab) + 1\n",
        "pos_size_hindi = len(hindi_pos_vocab) + 1\n",
        "\n",
        "# Model Architecture\n",
        "def create_rnn_model(embedding_dim, rnn_units, dropout_rate, max_len, vocab_size, pos_size):\n",
        "    # Word input branch\n",
        "    word_input = Input(shape=(max_len,))\n",
        "    word_embedding = Embedding(vocab_size, embedding_dim, input_length=max_len)(word_input)\n",
        "\n",
        "    # POS tag input branch\n",
        "    pos_input = Input(shape=(max_len,))\n",
        "    pos_embedding = Embedding(pos_size, embedding_dim, input_length=max_len)(pos_input)\n",
        "\n",
        "    # Concatenate word and POS tag embeddings\n",
        "    concatenated_input = Concatenate()([word_embedding, pos_embedding])\n",
        "\n",
        "    # RNN layer\n",
        "    rnn_layer = LSTM(rnn_units, return_sequences=True)(concatenated_input)  # You can try different RNN layers here\n",
        "\n",
        "    # Dropout layer\n",
        "    dropout_layer = Dropout(dropout_rate)(rnn_layer)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(pos_size, activation='softmax')(dropout_layer)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=[word_input, pos_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Split the datasets into training and testing sets\n",
        "english_X_train, english_X_test, english_y_train, english_y_test = train_test_split(english_train_seq, english_train_pos_seq, test_size=0.2)\n",
        "hindi_X_train, hindi_X_test, hindi_y_train, hindi_y_test = train_test_split(hindi_train_seq, hindi_train_pos_seq, test_size=0.2)\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 100\n",
        "rnn_units = 64\n",
        "dropout_rate = 0.2\n",
        "epochs = 10\n",
        "\n",
        "# Model Training and Evaluation\n",
        "# Create different RNN models and train them\n",
        "models = []\n",
        "\n",
        "# LSTM model\n",
        "lstm_model = create_rnn_model(embedding_dim, rnn_units, dropout_rate, english_max_len, vocab_size_eng, pos_size_eng)\n",
        "lstm_model.fit([english_X_train, english_y_train], english_y_train, epochs=epochs, validation_data=([english_X_test, english_y_test], english_y_test))\n",
        "models.append(('LSTM', lstm_model))\n",
        "\n",
        "# GRU model\n",
        "gru_model = create_rnn_model(embedding_dim, rnn_units, dropout_rate, english_max_len, vocab_size_eng, pos_size_eng)\n",
        "gru_model.fit([english_X_train, english_y_train], english_y_train, epochs=epochs, validation_data=([english_X_test, english_y_test], english_y_test))\n",
        "models.append(('GRU', gru_model))\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models:\n",
        "    print(f\"Evaluation for {name} Model:\")\n",
        "    # Evaluate on English dataset\n",
        "    english_loss, english_accuracy = model.evaluate([english_X_test, english_y_test], english_y_test)\n",
        "    print(f\"English - Loss: {english_loss}, Accuracy: {english_accuracy}\")\n",
        "    # Evaluate on Hindi dataset\n",
        "    hindi_loss, hindi_accuracy = model.evaluate([hindi_X_test, hindi_y_test], hindi_y_test)\n",
        "    print(f\"Hindi - Loss: {hindi_loss}, Accuracy: {hindi_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq4uuoe02KMi",
        "outputId": "4804d4f8-bd1b-4eb0-b0cc-4d84ea48a48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/indian.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "98/98 [==============================] - 24s 159ms/step - loss: 0.7992 - accuracy: 0.8970 - val_loss: 0.3604 - val_accuracy: 0.9194\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 7s 68ms/step - loss: 0.3423 - accuracy: 0.9177 - val_loss: 0.3022 - val_accuracy: 0.9207\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 5s 54ms/step - loss: 0.2827 - accuracy: 0.9271 - val_loss: 0.2497 - val_accuracy: 0.9395\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.2139 - accuracy: 0.9483 - val_loss: 0.1613 - val_accuracy: 0.9634\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.1278 - accuracy: 0.9720 - val_loss: 0.0926 - val_accuracy: 0.9814\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.0757 - accuracy: 0.9860 - val_loss: 0.0540 - val_accuracy: 0.9917\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 2s 21ms/step - loss: 0.0444 - accuracy: 0.9935 - val_loss: 0.0304 - val_accuracy: 0.9960\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 2s 21ms/step - loss: 0.0266 - accuracy: 0.9966 - val_loss: 0.0186 - val_accuracy: 0.9975\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 2s 22ms/step - loss: 0.0177 - accuracy: 0.9976 - val_loss: 0.0128 - val_accuracy: 0.9982\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 2s 22ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.0095 - val_accuracy: 0.9987\n",
            "Epoch 1/10\n",
            "98/98 [==============================] - 12s 99ms/step - loss: 0.8483 - accuracy: 0.8978 - val_loss: 0.3574 - val_accuracy: 0.9246\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 5s 56ms/step - loss: 0.3453 - accuracy: 0.9179 - val_loss: 0.3075 - val_accuracy: 0.9206\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 4s 38ms/step - loss: 0.2809 - accuracy: 0.9266 - val_loss: 0.2444 - val_accuracy: 0.9416\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.2076 - accuracy: 0.9502 - val_loss: 0.1586 - val_accuracy: 0.9636\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 2s 23ms/step - loss: 0.1270 - accuracy: 0.9727 - val_loss: 0.0921 - val_accuracy: 0.9829\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0747 - accuracy: 0.9866 - val_loss: 0.0525 - val_accuracy: 0.9915\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0437 - accuracy: 0.9930 - val_loss: 0.0307 - val_accuracy: 0.9948\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 0.0274 - accuracy: 0.9956 - val_loss: 0.0199 - val_accuracy: 0.9964\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 2s 21ms/step - loss: 0.0190 - accuracy: 0.9971 - val_loss: 0.0140 - val_accuracy: 0.9979\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 0.0138 - accuracy: 0.9982 - val_loss: 0.0102 - val_accuracy: 0.9988\n",
            "Evaluation for LSTM Model:\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9987\n",
            "English - Loss: 0.009524141438305378, Accuracy: 0.9986662864685059\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5189 - accuracy: 0.8931\n",
            "Hindi - Loss: 0.5189276933670044, Accuracy: 0.8930976390838623\n",
            "Evaluation for GRU Model:\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.9988\n",
            "English - Loss: 0.010182314552366734, Accuracy: 0.9988265633583069\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3927 - accuracy: 0.9113\n",
            "Hindi - Loss: 0.39272964000701904, Accuracy: 0.9112794399261475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzs-aJQz2VdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}